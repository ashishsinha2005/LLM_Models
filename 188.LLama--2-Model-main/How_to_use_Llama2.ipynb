{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Llama 2**"
      ],
      "metadata": {
        "id": "_TMB3gaVwcZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases."
      ],
      "metadata": {
        "id": "r4SR5jE6wemw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety."
      ],
      "metadata": {
        "id": "_L2RKR92wfjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"
      ],
      "metadata": {
        "id": "hucUDwz9wofJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`llama.cpp`'s objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n",
        "\n",
        "`GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage."
      ],
      "metadata": {
        "id": "2cEPsT2-wqtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Quantized Models from the Hugging Face Community"
      ],
      "metadata": {
        "id": "qPtdkKMQwxLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
        "\n",
        "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
        "\n",
        "We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n",
        "\n",
        "\n",
        "\n",
        "In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."
      ],
      "metadata": {
        "id": "olf6B7PXwzMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "1iyMrZUFw076"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k538LhZwWjL",
        "outputId": "e55ef5a9-e1b8-4435-bc0a-f05fb47032ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 21 17:27:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install the required packages"
      ],
      "metadata": {
        "id": "CnB1H40Gx57I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU llama-cpp-python\n",
        "\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install huggingface_hub\n",
        "!pip install numpy==1.23.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3F80q-emw8Ji",
        "outputId": "4ac169c8-13b2-411c-9834-1e68a83258f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.7 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=296593 sha256=4746c4acf6832dca0e84849d189fc03feddfb32d20b8c52caa689e45db273409\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "Collecting numpy==1.23.4\n",
            "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.4 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.4 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.23.4 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.4 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\n",
            "mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.23.4 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.23.4 which is incompatible.\n",
            "plotnine 0.14.1 requires numpy>=1.23.5, but you have numpy 1.23.4 which is incompatible.\n",
            "tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.4 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "50e7be8fa5a844b6a9fda1560d001c4c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Initialize the model"
      ],
      "metadata": {
        "id": "AUmk0Qqpx-Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q6_K.bin\""
      ],
      "metadata": {
        "id": "7XXuRcRFxZ9X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Import all required libraires"
      ],
      "metadata": {
        "id": "hlIjUOP9yDAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "A8njIJmsx4dE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Download the model"
      ],
      "metadata": {
        "id": "x6dF7nb5ySI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=model_name, filename=model_basename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVAOGLn1yMIo",
        "outputId": "bbbb38c3-e1f1-417a-d7f4-03116a6a3aff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AQSCgq1oyZUH",
        "outputId": "4568a91f-dab9-4d26-a3e6-ad6d0dc0ad65"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q6_K.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Load the Model"
      ],
      "metadata": {
        "id": "gF7A4y2QzqWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,#cpu_cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbGRlWcrzpR5",
        "outputId": "28e6a44f-8b65-448a-cacb-4d95cf2d3dd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to see the number of layes in GPU\n",
        "lcpp_llm.params.n_gpu_layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsgDmu_w1U6k",
        "outputId": "d774431d-6216-4012-e553-160d53593406"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Create Prompt Template"
      ],
      "metadata": {
        "id": "q7tigH4C1xKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a Poem about India Like how shakespeare wrote it\"\n",
        "prompt_template = f\"\"\" System: You are a helpfull assistant. Answer in a relavent way\n",
        "\n",
        "User: {prompt}\n",
        "Assistant :\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4iWHr_P_1wWW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Generating the Response"
      ],
      "metadata": {
        "id": "OjuHRieI2hbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = lcpp_llm(prompt = prompt_template,\n",
        "                    max_tokens=256,\n",
        "                    temperature=0.5,\n",
        "                    top_p=0.95,\n",
        "                    repeat_penalty=1.2,\n",
        "                    top_k=50,\n",
        "                    echo=True)"
      ],
      "metadata": {
        "id": "Ymw5TxLU2XHG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY5CWxwl22ip",
        "outputId": "5cd4393f-9718-450e-ea9a-c74c6d0ecc43"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-910a2599-778b-4e62-becc-20f10449f724', 'object': 'text_completion', 'created': 1732288976, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q6_K.bin', 'choices': [{'text': \" System: You are a helpfull assistant. Answer in a relavent way\\n\\nUser: Write a Poem about India Like how shakespeare wrote it\\nAssistant :  \\n\\nOh, fairest India, land of spices and gold,\\nWhere ancient temples rise to touch the sky so bold,\\nA land where culture meets the modern age,\\nWhere technology doth flourish in every stage.\\n\\nThy people, oh so diverse and bright,\\nWith customs, languages, and beliefs so tight,\\nFrom the snow-capped mountains to the sea so blue,\\nThis land of contrasts, a tale anew.\\n\\nIn thy bustling cities, a world so grand,\\nSkyscrapers pierce the sky, a modern land,\\nYet in the villages, tradition doth reign,\\nA glimpse into history's ancient refrain.\\n\\nThe Taj Mahal, a wonder to behold,\\nA symbol of love that time doth unfold,\\nThe Ganges River, where souls are cleansed,\\nA sacred place, where the divine is disclosed.\\n\\nOh India, land of mystery and might,\\nThy secrets hidden deep within thy sight,\\nA land so vast, a story to be told,\\nA tale of diversity, a treasure to behold.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 41, 'completion_tokens': 252, 'total_tokens': 293}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpCVbP8w25MW",
        "outputId": "b3048558-ecda-482c-f3e8-415673f81a05"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " System: You are a helpfull assistant. Answer in a relavent way\n",
            "\n",
            "User: Write a Poem about India Like how shakespeare wrote it\n",
            "Assistant :  \n",
            "\n",
            "Oh, fairest India, land of spices and gold,\n",
            "Where ancient temples rise to touch the sky so bold,\n",
            "A land where culture meets the modern age,\n",
            "Where technology doth flourish in every stage.\n",
            "\n",
            "Thy people, oh so diverse and bright,\n",
            "With customs, languages, and beliefs so tight,\n",
            "From the snow-capped mountains to the sea so blue,\n",
            "This land of contrasts, a tale anew.\n",
            "\n",
            "In thy bustling cities, a world so grand,\n",
            "Skyscrapers pierce the sky, a modern land,\n",
            "Yet in the villages, tradition doth reign,\n",
            "A glimpse into history's ancient refrain.\n",
            "\n",
            "The Taj Mahal, a wonder to behold,\n",
            "A symbol of love that time doth unfold,\n",
            "The Ganges River, where souls are cleansed,\n",
            "A sacred place, where the divine is disclosed.\n",
            "\n",
            "Oh India, land of mystery and might,\n",
            "Thy secrets hidden deep within thy sight,\n",
            "A land so vast, a story to be told,\n",
            "A tale of diversity, a treasure to behold.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a python program to develope a Machine Learning model using Random forest General code or lets take an iris dataset.\"\n",
        "prompt_template = f\"\"\" System: You are a helpfull assistant. Answer in a relavent way\n",
        "\n",
        "User: {prompt}\n",
        "Assistant :\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5tMS88X229pi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = lcpp_llm(prompt = prompt_template,\n",
        "                    max_tokens=256,\n",
        "                    temperature=0.5,\n",
        "                    top_p=0.95,\n",
        "                    repeat_penalty=1.2,\n",
        "                    top_k=50,\n",
        "                    echo=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWwRHxl-cuUU",
        "outputId": "16d8c9f7-a92c-48ef-f318-a7de95b2e945"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW9Hf06WczX7",
        "outputId": "6dd113e2-e5f0-4575-9c4a-68f2e758e5ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " System: You are a helpfull assistant. Answer in a relavent way\n",
            "\n",
            "User: Write a python program to develope a Machine Learning model using Random forest General code or lets take an iris dataset.\n",
            "Assistant :  \n",
            "\n",
            "Sure, I can assist you with that! To develop a machine learning model using random forests, we will use the scikit-learn library in Python. Here is some general code to get started:\n",
            "```\n",
            "import pandas as pd\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "# Load the iris dataset\n",
            "iris = pd.read_csv('iris.csv')\n",
            "\n",
            "# Preprocess the data\n",
            "X = iris.drop(['class'], axis=1)  # features\n",
            "y = iris['class']  # target variable\n",
            "\n",
            "# Split the data into training and testing sets\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
            "\n",
            "# Train a random forest classifier on the training set\n",
            "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
            "clf.fit(X_train, y_train)\n",
            "\n",
            "# Evaluate the model on the testing set\n",
            "accur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3vKYqlvgQeA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}